<!--
 How to run
 -->
<chapter id="How_to_run">
    <title>How to run</title>
    <chapterinfo>
    </chapterinfo>
    <sect1 id="Environment">
        <title>Environment</title>
        <para>
            In order to enable PoD's environment you need to source the <filename>PoD_env.sh</filename> script.
            The script is located in the directory where you installed PoD.
<screen>
<command>cd <replaceable>[POD INSTALLATION]</replaceable></command>
<command>source PoD_env.sh</command>
</screen>
            Also don't forget, before starting PoD the ROOT should be in the PATH as well.
        </para>
    </sect1>
    
    <sect1 id="Server">
        <title>Server</title>
        <note><title>local and remote PoD servers</title>
            <para>
            There are two ways you can use PoD: as a local server and a remote one.
            On how to use a remote PoD server please check the <xref linkend="pod-remote"/> command.
            </para>
            <para>
            Further in this chapter are the instructions on how to use a local PoD server.
        </para></note>
            <para>
                Use the <link linkend="pod-server">pod-server</link> command to start/stop/status PoD servers.
<screen>
<command>pod-server <parameter class='command'>start</parameter></command>
</screen>
            </para>
    </sect1>
    
    <sect1 id="JobManager">
        <title>Job Manager</title>
        <para>
            The next step is to submit remote PoD workers using PoD's job manager. These PoD workers will automatically setup your PROOF workers on
            remote hosts.
            Starting from version 2.0.7 the PoD project supports plug-ins. To submit remote jobs job manager plug-ins are used.
            That means PoD could be used on different resources like Grid, Cloud, RMS or just simple machines with only an ssh access on them.
            It also possible to use a combination of plug-ins to get PROOF workers on Grid worker nodes and local batch machines in the same time.
        </para>
        <para>
            In order to setup a dynamic PROOF cluster on RMS such as gLite, LSF, PBS, Condor, LoadLeveler or (Oracle/Sun)GridEngine,
            use the <link linkend="pod_submit">pod-submit</link> command.
        </para>
        <para>
            The following simple example illustrates a submission of 15 workers to an LSF farm using the "proof" queue.
<screen>
<command>pod-submit <parameter class='command'>-r</parameter> lsf <parameter class='command'>-q</parameter> proof <parameter class='command'>-n</parameter> 15</command>
</screen>
        </para>
        <para>
            Use PoD user defaults to tune individual settings of plug-ins: 
            gLite, <link linkend="LSF_CFG_TABLE">LSF</link>, <link linkend="PBS_CFG_TABLE">PBS</link>,
            <link linkend="Condor_CFG_TABLE">Condor</link>, LoadLeveler or <link linkend="OGE_CFG_TABLE">(Oracle/Sun)GridEngine</link>.
        </para>
        <para>
            If there is no RMS available, you can use PoD's SSH plug-in. Please see <xref linkend="SSH_plugin"/> for more details.
        </para>
    </sect1>
    
    <sect1 id="PROOF_workers">
        <title>PROOF workers</title>
            <para>
                As soon as a single job reaches remote worker node (WN), it tries to connect to PoD server to transfer information
                about itself and environment of WN. When negotiations are done and PoD server accepts WNs, it became a normal
                PROOF Worker for the user.
            </para>
            <para>
                It is not required to wait until all requested workers will be connected. Users could start analysis after
                reasonable number of workers are on-line, even after the first connected worker one could start the analysis.
                When other workers arrive, the ROOT (PROOF) session must be restarted in order to reconnect to the newly
                arrived workers.
            </para>
            <tip><para>
                PoD supports reconnection. That means if your analysis has a bug or a root session crashed you don't
                need to resubmit PoD jobs. You just need to close current root session, open it again. PoD will manage
                reconnection with its worker nodes automatically. Worker nodes will be on-line until the pod-agent service is on-line or
                until s Grid and/or batch queue time is over.
            </para></tip>
            <para>
                Use the <link linkend="pod-info">pod-info</link> command to find out a number
<screen>
<command>pod-info <parameter class='command'>-n</parameter></command>
</screen>
                or to list
<screen>
<command>pod-info <parameter class='command'>-l</parameter></command>
</screen>
                available PROOF workers.
            </para>
            <para>
                The <command>pod-info <parameter class='command'>-l</parameter></command> command can be also used to check,
                whether a direct connection (preferable) or
                a packet forwarded connection is used to connect PROOF server to workers.
                PoD y default automatically chooses the type of connections.
            </para>
            <para>
                If PoD server can't directly connect to its workers on xpd port, then the packet forwarded connection is used. With this type of connection,
                some PROOF functions will be limited. For example, workers can't be used as parallel sub-mergers, since a direct connection between workers will be required.
                We therefore recommend to open an xpd port range (PoD user defaults: worker.xproof_ports_range_(min/max)) for incoming connections on the worker nodes. This will help PoD to set up the most efficient type of connection.
            </para>        
    </sect1>
        
    
    <sect1 id="PROOF_Connection_String">
        <title>PROOF Connection String</title>
        <para>
        PROOF connection string - is an URL which is used as a parameter to the TProof::Open method. This URL actually
        contains an address of PROOF master, its host and port.
        </para>
        <para>
            Every time PoD is restarted it uses its automatic port mapping machinery to assign TCP ports to xproofd and other daemons.
            That means, a PROOF master port can always be a different one.
	    In order always get the actual port and even the whole PROOF connection string the <xref linkend="pod-info"/> can be used.
            </para>
            <para>
               For an example analysis, please see <xref linkend="How_to_test"/>.
            </para>
    </sect1>
    
    
    <sect1 id="Analysis">
        <title>Analysis</title>
        <para>
            Now when your remote PROOF workers (PoD workers) are on-line, you can process you ROOT/PROOF
            analysis normally, if it would be a usual PROOF session.
        </para>
        <para>
            For an example analysis, please see <xref linkend="How_to_test"/>.
        </para>
    </sect1>
    
    <sect1 id="How_to_shut_down_PoD">
        <title>How to shut down PoD</title>
            <para>
                In order to shut down PoD, a PoD server should be stopped.
<screen>
<command>pod-server <parameter class='command'>stop</parameter></command>
</screen>
            </para>
    </sect1>
    <sect1><title>if something is wrong</title>
        <para>
            If something goes wrong, something doesn't work as expected, please, check the log files first.
            <table id="POD_LOG_FILES">
                <title>PoD log files</title>
                <tgroup cols="3">
                    <thead>
                        <row>
                            <entry>Name</entry>
                            <entry>Location</entry>
                            <entry>Description</entry>
                        </row>
                    </thead>
                    <tbody>		
                        <row>
                            <entry>pod-agent.server.log</entry>
                            <entry><xref linkend="server.logfile_dir"/>/pod-agent.server.log</entry>
                            <entry>This file contains a log messages of the pod-agent, which runs on the user interface.</entry>
                        </row>
                        <row>
                            <entry>xpd.log</entry>
                            <entry><xref linkend="server.logfile_dir"/>/PoDServer</entry>
                            <entry>This is an XROOTD log file.</entry>
                        </row>
                    </tbody>
                </tgroup>
            </table>
            All job manager plug-ins are also able to deliver the logs from worker nodes. Please refer to the plug-ins configuration for more details.
        </para>
        <para>
            If you still can't resolve the issue or have something to report, use <xref linkend="Support"/>.
        </para>
    </sect1>
    
</chapter>

